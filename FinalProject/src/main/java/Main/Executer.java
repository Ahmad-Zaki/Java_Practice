package Main;

import java.util.Arrays;
import java.util.Collection;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.Map.Entry;
import java.util.function.Function;
import java.util.stream.Collectors;

import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;

public class Executer {
	private JobsDAO jobsDAO = new JobsDAO("Wuzzuf Jobs dataset Analysis","local[12]");
	private Dataset<Row> jobs;

	public Executer() {
		//omit the log generated by apache spark from the output.
    	Logger.getLogger("org").setLevel(Level.ERROR);
    	// Set reader options as a map object:
		Map<String,String> readerOptions = new HashMap<String, String>();
    	readerOptions.put("header", "true");	//set the first row as the header.
    	readerOptions.put("delimiter", ",");	//set the delimiter as the comma ","
    	//Read the dataset from CSV file:
    	jobs = jobsDAO.CSVtoDataset("src\\main\\resources\\Wuzzuf_Jobs.csv",readerOptions);
	}

	
	public String showJobs() {
    	//Display the first few rows of the data:
    	return jobs.showString(10, 30, false);
	}
	
	public String descripeData() {
		return jobs.describe(jobs.columns()).showString(10, 20, false) +"\n\n"+ "Number of entries= " + jobs.count();
	}
	
	public String cleanData() {
		Dataset<Row> jobsCleaned = jobsDAO.cleanJobs();
		return jobsCleaned.describe(jobs.columns()).showString(10, 20, false) +"\n\n"+ "Number of entries after cleaning= " + jobsCleaned.count();
	}
	
	public String companyCount() {
		//Count each company and sort them in descending order:
    	Dataset<Row> companiesCount = jobsDAO.countAttribute("Company");
    	return companiesCount.showString(10, 30, false);
	}
	
	public String jobsCount() {
		//Count each job and sort them in descending order:
		Dataset<Row> jobsCount = jobsDAO.countAttribute("Title");
    	return jobsCount.showString(10, 30, false);
	}
	
	public String areaCount() {
		//Count each area and sort them in descending order:
		Dataset<Row> locationCount = jobsDAO.countAttribute("Location");
    	return locationCount.showString(10, 30, false);
	}
	
	public String factorize() {
		//Count each area and sort them in descending order:
		Dataset<Row> factorizedDF = jobsDAO.factorize("YearsExp");
    	return factorizedDF.showString(10, 25, false);
	}
	
	public String skillCount() {
		
		
		Dataset<Row> jobsCleaned = jobsDAO.cleanJobs();
		//Get a list of all skills listed the dataset:
		List<String> allSkills = jobsCleaned.select("Skills")	//Select the "Skills" column.
											.collectAsList()	//Return the column as a list.
											.stream()
											.map(row -> ((String) row.get(0)).strip().toLowerCase().split(",", 0)) //get an array of skills per title.
											.map(Arrays::asList)				//get skills per title as list.
											.flatMap(Collection::stream)		// stream skills per title to collect all skills in one list.
											.collect(Collectors.toList());
		
		
		//Count the skills to see the most important skill:
		Set<Entry<String, Long>> skillsCount = allSkills.stream()
													    .collect(Collectors.groupingBy(Function.identity(),Collectors.counting()))
													    .entrySet();
		
		//Sort and get top 10 skills with their count
		List<Entry<String, Long>> topSkills = skillsCount.stream()
									    			     .sorted((e1,e2) ->  Long.compare(e2.getValue(), e1.getValue()))
									    			     .limit(10)
									    		  	     .collect(Collectors.toList());
		
		return topSkills.toString().replaceAll(",", "\n	").replace('[', ' ').replace(']', ' ');
    	
		
	}
	
	 
	
}
